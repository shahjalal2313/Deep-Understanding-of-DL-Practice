{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac29522c",
   "metadata": {},
   "source": [
    "# Deep Understanding of Deep Learning Practice\n",
    "## Chapter: Math,Numpy,Pytorch\n",
    "### Topic: Entropy and cross-Entropy\n",
    "#### Shahjalal Shanto\n",
    "##### Department of Chemistry, University of Chittagong\n",
    "**24 February 2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9d858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12614c1a",
   "metadata": {},
   "source": [
    "**Note**\n",
    "* Entropy is a measure that describes the amount of uncertainity about a specific variable\n",
    "* Entropy is maximum at probability ,p= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b77e4",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"> Formula of entropy </h3>\n",
    "\n",
    "$$\n",
    "  H(p) = -\\sum_{i=1}^n p(x_i)log_2(p(x_i))\n",
    "$$\n",
    "* x = data values\n",
    "* p = probability\n",
    "* the negative sign is becuase the value of log is negative and probability values cant be negative so to cancel out this - sign is used\n",
    "* **Importance:** \n",
    "High entropy means that the dataset has a lot of variability. Low entropy means that most of the values of the dataset repeat and therefore redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5030ffe",
   "metadata": {},
   "source": [
    "**How Variance and Entropy differ?**\n",
    "* Entropy is a non-linear and makes no assumption about the distribution.\n",
    "* Variance depends on the validity of the mean and therefore is appropriate for roughly normal data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2ca08",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"> Cross entropy </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd08349",
   "metadata": {},
   "source": [
    "* Entropy describes one Probability distribution\n",
    "* Cross entropy describes the relationship between two probability distribution\n",
    "**Formula of Cross Entropy**\n",
    "\n",
    "$$H(p,q) = -\\sum plog(q)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bc911",
   "metadata": {},
   "source": [
    "* Here p is one distribution and q is another distribution\n",
    "* In DL, the way cross entropy is used is to Characterizes the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f411ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Entropy: 0.34657359027997264\n"
     ]
    }
   ],
   "source": [
    "#probability of a event happening \n",
    "p = 0.25\n",
    "#not the correct formula \n",
    "H = - (p*np.log(p))\n",
    "print(f\"Wrong Entropy: {H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e23b6",
   "metadata": {},
   "source": [
    "**This measurements of entropy is wrong becuase it only calculates the probability of the event happening ignoring calculating probability of event not happennig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310c70b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct entropy: 0.5623351446188083\n"
     ]
    }
   ],
   "source": [
    "#the correctway\n",
    "p= .25\n",
    "pC = 1-.25\n",
    "#print(pC)\n",
    "x= [p,pC]\n",
    "\n",
    "H=0\n",
    "for i in x:\n",
    "    H -= i*np.log(i)\n",
    "\n",
    "print(f'Correct entropy: {H}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5622d296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5623351446188083"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to corrcetly represents for N=2 events\n",
    "H = -(p*np.log(p)+(1-p)*(np.log(1-p)))\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bec3e",
   "metadata": {},
   "source": [
    "**This form is used in binary entropy. which is one of the most important loss function in DL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6152e1b",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f08b4",
   "metadata": {},
   "source": [
    "**Note** All probs must sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a129428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "p = [1,0]\n",
    "q = [.25,.75]\n",
    "\n",
    "H = 0 \n",
    "for i in range(len(p)):\n",
    "    H -= p[i]*np.log(q[i])\n",
    "    \n",
    "print(f'Cross entropy: {H}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41f9a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct entropy: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "#binary cross-entropy \n",
    "H = -(p[0]*np.log(q[0]) + p[1]*np.log(q[1]))\n",
    "print(f'Correct entropy: {H}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5640c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified H: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "# as the value of p =0,1, we can further simplied it to \n",
    "H = -np.log(q[0])\n",
    "print(f'Simplified H: {H}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ba7d0",
   "metadata": {},
   "source": [
    "## Calculating Binary Cross entropy Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16040ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8e65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import must be tensor\n",
    "p_tensor = torch.Tensor(p)\n",
    "q_tensor = torch.Tensor(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e971d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3863)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating binary Cross entropy\n",
    "F.binary_cross_entropy(q_tensor,p_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea35ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(75.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Order matters, \n",
    "F.binary_cross_entropy(p_tensor,q_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
